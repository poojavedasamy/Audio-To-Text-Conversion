from google.colab import files
import os
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torch
import librosa
uploaded = files.upload()
audio_file = list(uploaded.keys())[0]
if not os.path.exists(audio_file):
    raise FileNotFoundError(f"Audio file not found at {audio_file}")
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
audio_input, sr = librosa.load(audio_file, sr=16000)
input_values = processor(audio_input, return_tensors="pt").input_values
with torch.no_grad():
    logits = model(input_values).logits
predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.decode(predicted_ids[0])
print("Transcription:", transcription)
